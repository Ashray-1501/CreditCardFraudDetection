{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle as pkl\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from feature_engine.encoding import MeanEncoder\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "# trans_date_trans_time to pandas datetime\n",
    "    data['trans_date_trans_time'] = pd.to_datetime(data['trans_date_trans_time'])\n",
    "    data['trans_date_trans_time'].head(3)\n",
    "\n",
    "    data['dob'] = pd.to_datetime(data['dob'])\n",
    "    data['dob'].head(3)\n",
    "\n",
    "    drop_cols = ['street','merchant','zip','first','last','trans_num','job'] # list of columns to be dropped\n",
    "    data.drop(drop_cols, axis =1, inplace = True)\n",
    "    list(data.columns)\n",
    "\n",
    "    data['trans_hour'] = data['trans_date_trans_time'].dt.hour  # extracting the hour component using the dt accessor\n",
    "\n",
    "    data['trans_hour'].unique() # printing the unique values in the extracted series\n",
    "\n",
    "\n",
    "    data['trans_month'] = data['trans_date_trans_time'].dt.month # extracting the month number component using the dt accessor\n",
    "\n",
    "    data['trans_month'].unique() # printing the unique values in the extracted series\n",
    "\n",
    "\n",
    "    data['trans_dayofweek'] = data['trans_date_trans_time'].dt.day_name() # extracting the day name component using the dt accessor\n",
    "\n",
    "    data['trans_dayofweek'].unique() # printing the unique values in the extracted series\n",
    "\n",
    "\n",
    "    # data.groupby(['cc_num'])['cc_num'].count().sort_values(ascending = False).describe().astype(int)\n",
    "\n",
    "\n",
    "    data.sort_values(by = ['cc_num','unix_time'], ascending = True, inplace = True)\n",
    "\n",
    "# unix_time for the previouse transaction using the shift method in pandas\n",
    "\n",
    "    data['unix_time_prev_trans'] = data.groupby(by = ['cc_num'])['unix_time'].shift(1)\n",
    "\n",
    "    # For the first transactions-records all the credit cards, the previouse unit time will be null\n",
    "    # we dont want any null values to be present in the variable as we are going to feed the dataset into machine learning models where null values are not expected\n",
    "    # for all the rows with null values, we are filling with the current unit time value - 86400 (number of seconds in a day)\n",
    "\n",
    "    data['unix_time_prev_trans'].fillna(data['unix_time'] - 86400, inplace = True)\n",
    "\n",
    "    # calculatig the time delay between the previouse and current transaction - converting the variable into to mins\n",
    "\n",
    "    data['timedelta_last_trans'] = (data['unix_time'] - data['unix_time_prev_trans'])//60\n",
    "\n",
    "\n",
    "    data['dob'].head()\n",
    "\n",
    "    \"\"\"> calculating the age at the date of the transaction = `dob` - `trans_date_trans_time`\"\"\"\n",
    "\n",
    "    data['cust_age'] = (data['trans_date_trans_time'] - data['dob']).astype('timedelta64[Y]') # calculting the age in days and converting it into years\n",
    "\n",
    "    data['cust_age'].head() # lets look at the newly arrived age column\n",
    "\n",
    "\n",
    "\n",
    "    data['lat_dist_cust_merch'] = (data['lat'] -data['merch_lat']).abs()\n",
    "    data['lat_dist_cust_merch'].head(3)\n",
    "\n",
    "    \"\"\"> Calculate the long distance between the customer and current merchant\"\"\"\n",
    "\n",
    "    data['long_dist_cust_merch'] = (data['long'] -data['merch_long']).abs()\n",
    "    data['long_dist_cust_merch'].head(3)\n",
    "\n",
    "    \"\"\"> Get the lat and long values of the previouse merchant\"\"\"\n",
    "\n",
    "    data['prev_merch_lat'] = data.groupby(by = ['cc_num'])['merch_lat'].shift(1) # latitude of the previouse merchant with pandas shift method\n",
    "\n",
    "    data['prev_merch_long'] = data.groupby(by = ['cc_num'])['merch_long'].shift(1) # longitude of the previouse merchant with pandas shift method\n",
    "\n",
    "    \"\"\"> Fill the null values ( for all initial transctions 999 numbers ) with the lat long values of the current merchant\"\"\"\n",
    "\n",
    "    data['prev_merch_lat'].fillna(data['merch_lat'], inplace = True)\n",
    "\n",
    "    data['prev_merch_long'].fillna(data['merch_long'], inplace = True)\n",
    "\n",
    "    \"\"\"> Calculate the distnace between the current and the previouse merchant\"\"\"\n",
    "\n",
    "    data['lat_dist_prev_merch'] = (data['merch_lat'] - data['prev_merch_lat']).abs() # calculate and convert into absolute value\n",
    "\n",
    "    data['lat_dist_prev_merch'].head(3) # lets look at the newly arrived variable\n",
    "\n",
    "    \"\"\"> Calculate the distnace between the current and the previouse merchant\"\"\"\n",
    "\n",
    "    data['long_dist_prev_merch'] = (data['merch_long'] -data['prev_merch_long']).abs() # calculate and convert into absolute value\n",
    "\n",
    "    data['long_dist_prev_merch'].head(3) # lets look at the newly arrived variable\n",
    "\n",
    "\n",
    "\n",
    "    drop_cols2 = ['trans_date_trans_time','cc_num','unix_time','unix_time_prev_trans','lat',\n",
    "                'long','merch_lat','merch_long','prev_merch_lat','prev_merch_long','dob','city']\n",
    "\n",
    "    \"\"\"> Dropping the list of columns which are now redundant in the dataset\"\"\"\n",
    "\n",
    "    data.drop(drop_cols2, axis = 1, inplace = True)\n",
    "    data.reset_index(drop=True, inplace = True)\n",
    "    list(data.columns) # lets look at the remaining list of columns\n",
    "\n",
    "\n",
    "    X_test = data\n",
    "\n",
    "    capper_iqr = pkl.load(open(\"pickles/capper_iqr.pkl\",\"rb\"))\n",
    "    X_test = capper_iqr.transform(X_test) # tranforming the test X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    onehot_encod = pkl.load(open(\"pickles/onehod_encod.pkl\",\"rb\"))\n",
    "\n",
    "    X_test = onehot_encod.transform(X_test) # transform test X\n",
    "\n",
    "\n",
    "    variables = ['state','trans_dayofweek']\n",
    "\n",
    "\n",
    "\n",
    "    # mean_encod = MeanEncoder(variables = variables)\n",
    "    mean_encod = pkl.load(open(\"pickles/mean_encod-1.pkl\",\"rb\"))\n",
    "\n",
    "    # mean_encod.fit(X_test,y_test)\n",
    "\n",
    "    X_test = mean_encod.transform(X_test) # Transforming the X test\n",
    "\n",
    "    X_test['state'].unique()\n",
    "\n",
    "    yeojohnson_transformer = pkl.load(open(\"pickles/yeojohnson_transformer.pkl\",\"rb\"))\n",
    "\n",
    "    X_test = yeojohnson_transformer.transform(X_test) # Transforming the X test\n",
    "\n",
    "    scaler = pkl.load(open(\"pickles/scaler.pkl\",\"rb\"))\n",
    "\n",
    "    scaler.data_max_\n",
    "\n",
    "    scaler.data_min_\n",
    "\n",
    "    X_test = pd.DataFrame(data = scaler.transform(X_test), columns = X_test.columns) # transform the X test\n",
    "\n",
    "    logreg = pkl.load(open(\"pickles/logreg (1).pkl\",\"rb\"))\n",
    "    return logreg.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    " '2020-06-21 12:14:25',\n",
    " 2291163933867244,\n",
    " 'fraud_Kirlin and Sons',\n",
    " 'personal_care',\n",
    " 2.86,\n",
    " 'Jeff',\n",
    " 'Elliott',\n",
    " 'M',\n",
    " '351 Darlene Green',\n",
    " 'Columbia',\n",
    " 'SC',\n",
    " 29209,\n",
    " 33.9659,\n",
    " -80.9355,\n",
    " 333497,\n",
    " 'Mechanical engineer',\n",
    " '1968-03-19',\n",
    " '2da90c7d74bd46a0caf3777415b3ebd3',\n",
    " 1371816865,\n",
    " 33.986391,\n",
    " -81.200714,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['trans_date_trans_time', 'cc_num', 'merchant', 'category',\n",
    "       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n",
    "       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n",
    "       'merch_lat', 'merch_long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-06-21 12:14:25</td>\n",
       "      <td>2291163933867244</td>\n",
       "      <td>fraud_Kirlin and Sons</td>\n",
       "      <td>personal_care</td>\n",
       "      <td>2.86</td>\n",
       "      <td>Jeff</td>\n",
       "      <td>Elliott</td>\n",
       "      <td>M</td>\n",
       "      <td>351 Darlene Green</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>...</td>\n",
       "      <td>29209</td>\n",
       "      <td>33.9659</td>\n",
       "      <td>-80.9355</td>\n",
       "      <td>333497</td>\n",
       "      <td>Mechanical engineer</td>\n",
       "      <td>1968-03-19</td>\n",
       "      <td>2da90c7d74bd46a0caf3777415b3ebd3</td>\n",
       "      <td>1371816865</td>\n",
       "      <td>33.986391</td>\n",
       "      <td>-81.200714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  trans_date_trans_time            cc_num               merchant  \\\n",
       "0   2020-06-21 12:14:25  2291163933867244  fraud_Kirlin and Sons   \n",
       "\n",
       "        category   amt first     last gender             street      city  \\\n",
       "0  personal_care  2.86  Jeff  Elliott      M  351 Darlene Green  Columbia   \n",
       "\n",
       "   ...    zip      lat     long  city_pop                  job         dob  \\\n",
       "0  ...  29209  33.9659 -80.9355    333497  Mechanical engineer  1968-03-19   \n",
       "\n",
       "                          trans_num   unix_time  merch_lat  merch_long  \n",
       "0  2da90c7d74bd46a0caf3777415b3ebd3  1371816865  33.986391  -81.200714  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_data = {}\n",
    "for x in range(0,len(test_data)):\n",
    "    dict_data[columns[x]] = [test_data[x]]\n",
    "newDF = pd.DataFrame(dict_data)\n",
    "newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['street','merchant','zip','first','last','trans_num','job'] # list of columns to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(newDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
